{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attractive-court",
   "metadata": {},
   "source": [
    "# Interpreting Parity Measures\n",
    "\n",
    "This lecture will follow the treatment in *A Moral Framework for Understanding of Fair ML through Economic Models of Equality of Opportunity* {cite}`heidari_2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-alcohol",
   "metadata": {},
   "source": [
    "This section presents one possible translation of the parity criteria from the last lecture into the frameworks of Rawlsian Equality of Opportunity and Luck Egalitarianism. This translation is only one possible attempt at mapping these concepts and does so in an *observational* way. It attempts to make clear underlying assumptions of fairness in these observational criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-mailman",
   "metadata": {},
   "source": [
    "## Review of Frameworks for Fairness\n",
    "\n",
    "We briefly review the (quantitative) definitions for Rawlsian Equality of Opportunity and Luck Egalitarianism given in lecture 2.\n",
    "\n",
    "Suppose that:\n",
    "* $c$ stands for circumstances that capture factors beyond one's control (e.g. circumstances of birth, or that brought on by luck). \n",
    "* $e$ are actions that follow from ones choices and character ('effort'). \n",
    "* $u$ is the utility \n",
    "* An (allocative) policy $\\phi$ induces a distribution of utility among a population. \n",
    "* $F^\\phi(.|c, e)$ is the cumulative distribution of utility under policy $\\phi$, at fixed effort level $e$ and circumstance $c$.\n",
    "\n",
    "**Rawlsian Equality of Opportunity** can be translated into the language above as follows. We say a policy $\\phi$ satisfies Rawlsian Equality of Opportunity if for all circumstances $c,c'$ and all efffort levels $e$,\n",
    "\n",
    "$$\n",
    "F^\\phi(.|c,e) = F^\\phi(.|c', e)\n",
    "$$\n",
    "\n",
    "This notion supposes a view of effort that makes sense to compare between any two individuals. Effort is inherent to an individual and not impacted by circumstance $c$. Also recall that effort captures a lot more than simple effort (according to Rawls, it captures talent, ambition and other natural characteristics as effort).\n",
    "\n",
    "**Luck Egalitarian Equality of Opportunity** can be translated into the language above as follows. Suppose that $\\pi$ is the $\\pi$th quantile of the distribution of effort of individuals under circumstance $c$. Then a policy  satisfies Luck Egalitarian EO if for all $\\phi\\in[0,1]$ and any two circumstances $c, c'$\n",
    "\n",
    "$$\n",
    "F^\\phi(.|c,\\pi) = F^\\phi(.|c', \\pi)\n",
    "$$\n",
    "\n",
    "In the deciding admission to college, this would be like comparing those at top 10% of their high-school class. Students at different high schools likely have different absolute academic credentials. This policy considers the environment (circumstance) the student came from when comparing effort levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-webcam",
   "metadata": {},
   "source": [
    "## Translating the Components of an Algorithmic Decision\n",
    "\n",
    "Now we will translate the formalism above into the context of an algorithmic decision making system. Consider the notation from the previous lecture:\n",
    "\n",
    "* $X$ are relevant attributes (e.g. those reasonable for making decisions)\n",
    "* $A$ are irrelevant attributes (e.g. race)\n",
    "* $Y$ is the outcome or *true label*.\n",
    "* $\\hat{Y} = C(X, A)$ is an allocation by a classifier.\n",
    "\n",
    "The utility that an algorithmic decision imparts may be described as follows:\n",
    "* $d\\in[0,1]$ is an individual's *effort-based utility*. This is the utility they receive based on relevant factors made by the individual. This utility is not directly observable.\n",
    "* $a\\in[0,1]$ is the *actual utility* the individual receives subsequent to receiving allocation $\\hat{y}$.\n",
    "* $u$ is the utility or advantage the individual earns as a result of being subject to the decision $C$. We assume $u = a - d$ in these notes.\n",
    "\n",
    "The overall utility $u$ is what you want to equalize across individuals in similar circumstances. Note:\n",
    "* If $u = 0$, then $a = d$ and effort directly matches what's given.\n",
    "* If $u >0$, then $a > d$ and the individual benefits from the decision (beyond effort).\n",
    "* If $u<0$, then $a < d$ and the individual is harmed by the decision (taking effort into account).\n",
    "\n",
    "Equality of Opportunity, according to Heidari et al., requires individuals with similar effort have similar prospects of earning the advantage $u$. How similarity of effort is measured changes the specific conception of EO.\n",
    "\n",
    "We can translate our formalism about EO into the language of machine learning:\n",
    "\n",
    "|Eqality of Opportunity|Machine Learning|\n",
    "|---|---|\n",
    "|policy $\\phi$|classifier $C$|\n",
    "|effort $e$|effort-based utility $d$|\n",
    "|circumstance $c$|irrelevant attributes $A$|\n",
    "|utility $u$| overall utility $u = (a - d)|\n",
    "\n",
    "Note: by capturing circumstance with $A$, we are simplifying an individual notion of fairness to one about groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-comment",
   "metadata": {},
   "source": [
    "### Rawlsian FEO\n",
    "\n",
    "Rawlsian EO requires effort $d$ to not be affected by irrelevant features $A$ and the specific decision $C$. Let $F^C(.)$ be the distribution of utility $u$ across individuals under the allocative decisions made by $C$. The Rawlsian EOP translates to:\n",
    "\n",
    "$$\n",
    "F^C(.|A = a, D = d) = F^C(.|A = b, D = d)\n",
    "$$\n",
    "\n",
    "That is, for fixed effort $d$, the distribution of utility does not depend on circumstance $A$.\n",
    "\n",
    "Many of the parity criteria from the last lecture are naturally described using Rawlsian Fair Equality of Opportunity, for different choices of effort. This allows us to clearly describe the assumptions we make when requiring a specific parity measure to hold.\n",
    "\n",
    "**Equality of Odds**. If the true binary label $Y$ reflects an individual's effort-based utility $D$, then Rawlsian FEO translates to equality of odds across protected groups. That is:\n",
    "* $d = Y$\n",
    "* $a = \\hat{Y}$\n",
    "* $u = (Y - \\hat{Y})\n",
    "\n",
    "That is, equality of odds is a world where you receive exactly what you work for. Practically speaking however, if the true label identified in the training data doesn't perfectly reflect a persons effort, this condition no longer holds!\n",
    "\n",
    "**Demographic Parity** is framed as Rawlsian EO by defining the effort-based utility as constant of $d = 1$ and the actual utility to be the result of the decision $a = \\hat{Y}$. This implies that each person has some intrinsic constant utility that's independent of effort or circumstances. Such an assignment may be reasonable if you are considering the distribution of a good that equally deserved among all people (e.g. an inalienable right).\n",
    "\n",
    "**Accuracy Parity** is framed as Rawlsian EO by defining the effort-based utility as constant of $d = 0$ and the actual utility to be the discrepancy between the decision and the true label $(\\hat{Y} - Y)^2$. This implies that individual has no intrinsic utility and all that matters is the correctness of the outcome.\n",
    "\n",
    "The proofs of these equivalences are found in {cite}`heidari_2019`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-civilization",
   "metadata": {},
   "source": [
    "### Luck Egalitarianism\n",
    "\n",
    "With notation as before, define $\\pi$ to be the distribution of utility for individuals in  at the $\\pi$th quantile of the distribution of effort-based utility. Equalizing opportunities means choosing the predictive model $C$ to equalize the distribution of utility across types, at fixed levels of $\\pi$:\n",
    "\n",
    "$$\n",
    "F^C(.|A = a, \\Pi = \\pi) = F^C(.|A = b, \\Pi = \\pi)\n",
    "$$\n",
    "\n",
    "**Predictive Value Parity** is framed as Luck Egalitarianism, by equating effort-based utility with the classifiers decision $d = \\hat{Y}$ and actual utility with the true outcome $a = Y$.\n",
    "\n",
    "Here, the assumption made is that \"differences in actual outcomes among equally risky individuals is mainly driven by arbitrary factors, such as brute luck\"  {cite}`heidari_2019`. Thus predictive value parity assumes that the classifier's assessment is assumed to capture effort, while differences between effort in the actual outcome are primarily due to circumstance.\n",
    "\n",
    "*Remark*: Predictive Value Parity cannot be framed as Rawlsian, as it requires conditioning on the decisions made by the classifier; it requires equal rates of true outcomes across similar predictions.\n",
    "\n",
    "Different choices of $d$ and $a$ may lead to new parity measures that encode different values.  {cite}`heidari_2019` explores some of these at the end of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-approach",
   "metadata": {},
   "source": [
    "## Interpretations of trade-off using Equality of Opportunity\n",
    "\n",
    "Predictive value parity and equality of odds cannot hold simultaneously due to differences in moral assumptions encoded in effort-based utility $d$:\n",
    "* Equality of odds assumes persons with similar true labels are equally accountable for their labels.\n",
    "* Predictive value parity assumes all persons with the same 'risk' (predicted label) are equally accountable for their predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

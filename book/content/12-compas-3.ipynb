{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set(rc={'figure.figsize':(12,6)})\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "recidivism = (\n",
    "    pd.read_csv(url)\n",
    "    .drop(['name', 'first', 'last'], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-strike",
   "metadata": {},
   "source": [
    "# COMPAS Analysis\n",
    "\n",
    "### Individual Fairness\n",
    "\n",
    "* Define individual similarity measure on COMPAS\n",
    "* Investigate COMPAS w/r/t individual fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-usage",
   "metadata": {},
   "source": [
    "### Define a Metric\n",
    "\n",
    "For comparing similarity between individuals, use 'task relevant' variables:\n",
    "* Criminal History\n",
    "* Severity of current charge\n",
    "* Age of defendant (associated w/recidivism)\n",
    "\n",
    "'task relevant' variables reflect *effort*.\n",
    "\n",
    "To start:\n",
    "* Don't use Race, Gender\n",
    "* Use Euclidean Distance for numeric variables\n",
    "* Use \"Hard Match\" for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_vars = [\n",
    "    'age', 'juv_fel_count', 'juv_misd_count', \n",
    "    'priors_count', 'c_charge_degree', 'c_charge_desc']\n",
    "\n",
    "other_vars = ['decile_score', 'is_recid', 'race', 'sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-drunk",
   "metadata": {},
   "source": [
    "### Metric Base Class\n",
    "\n",
    "* On instantiation, gather column types (to know how to process them)\n",
    "* Define a `dist` method that takes in two rows, outputs a non-negative number (metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompasMetricBase(object):\n",
    "    \n",
    "    def __init__(self, df, metric_vars):\n",
    "        \n",
    "        self.vars = metric_vars\n",
    "        \n",
    "        df = df[metric_vars]\n",
    "        self.nums = df.dtypes.loc[lambda x:(x == 'float') | (x == 'int')].index\n",
    "        self.cats = df.dtypes.loc[lambda x:x == 'object'].index\n",
    "        return\n",
    "    \n",
    "        \n",
    "    def dist(self, ser1, ser2):\n",
    "        \n",
    "        # sum of squares\n",
    "        # dnum = np.sqrt(((ser1.loc[self.nums] - ser2.loc[self.nums])**2).sum())\n",
    "        \n",
    "        # absolute difference\n",
    "        dnum = np.abs((ser1.loc[self.nums] - ser2.loc[self.nums])).sum()\n",
    "        \n",
    "        # exact-match\n",
    "        dcat = (1 - (ser1.loc[self.cats] == ser2.loc[self.cats]).astype(int)).sum()\n",
    "        \n",
    "        return (dnum + dcat) / len(self.vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this\n",
    "def min_max_normalize(df):\n",
    "    \n",
    "    return (df - df.min()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = recidivism[metric_vars + other_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CompasMetricBase(df, metric_vars)\n",
    "# d = MaxSimilarityMetric(df, metric_vars)\n",
    "# d = CompasMetricBow(df, metric_vars)\n",
    "# d = CompasMetricLuckEgal(df, metric_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-robinson",
   "metadata": {},
   "source": [
    "### Compute pairwise similarities\n",
    "\n",
    "* Too many to compute exhaustively\n",
    "    - COMPAS is ~7k individuals\n",
    "    - Pairwise distances ~5.2M pairs\n",
    "* Procedure:\n",
    "    - Randomly sample 10k pairs\n",
    "    - Compute pairwise similarity of those pairs (`d`)\n",
    "    - Calculate pairwise similarity of outcomes (`D`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = 10000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "a = df.copy()\n",
    "a[d.nums] = min_max_normalize(a[d.nums]) # comment out (for luck egalitarianism)\n",
    "\n",
    "dm = []\n",
    "for i,j in np.random.choice(a.index, size=(samp,2)):\n",
    "    v1, v2 = a.loc[i], a.iloc[j]\n",
    "    dist = d.dist(v1, v2)\n",
    "    D = np.abs(v1['decile_score'] - v2['decile_score']) / 10\n",
    "    dm.append((i, j, dist, D))\n",
    "        \n",
    "dm = pd.DataFrame(dm, columns=['idx_1', 'idx_2', 'd', 'D'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-accused",
   "metadata": {},
   "source": [
    "### Results: pairs + (d, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-climate",
   "metadata": {},
   "source": [
    "### Select 'unfair' decisions based on (d,D)-Fairness condition\n",
    "\n",
    "The Lipschitz condition for fairness is:\n",
    "\n",
    "$$D(S(x), S(y)) \\leq d(x,y)$$\n",
    "\n",
    "* We calculate the proportion of pairs that don't satisfy the fair decision condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfairs = dm[dm['d'] < dm['D']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unfairs) / len(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "dm['d'].plot(kind='hist', bins=25, ax=ax1, title='distribution of similarity of individuals (d)')\n",
    "dm['D'].plot(kind='hist', ax=ax2, title='distribution of similarity of outcomes (D)')\n",
    "\n",
    "plt.suptitle('Distibutions of (d, D) in sample population')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "unfairs['d'].plot(kind='hist', bins=25, ax=ax1, title='distribution of similarity of individuals (d)')\n",
    "unfairs['D'].plot(kind='hist', bins=np.linspace(0,1,10), ax=ax2, title='distribution of similarity of outcomes (D)')\n",
    "\n",
    "plt.suptitle('Distibutions of (d, D) in among unfair decisions')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-movie",
   "metadata": {},
   "source": [
    "### Question: thinking about what goes into our metrics\n",
    "\n",
    "* How are variables individually weighted in the similarity calculation?\n",
    "    - which variable do you think has the most influence?\n",
    "    - you can train a regression model to learn `d` and look at the importance of each variable\n",
    "* Are `d` and `D` appropriately comparable?\n",
    "    - can you think of a choice of metric `d` or `D` so that fairness always holds?\n",
    "    - can you think of a choice of metric `D` so that fairness almost never holds?    \n",
    "* Return to top and make metric tweaks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "jittered = dm.assign(\n",
    "    D=(dm['D'] + np.random.normal(0,0.01, size=dm.shape[0]))\n",
    ")\n",
    "\n",
    "jittered.plot(kind='scatter', x='d', y='D', c='b', title='plot of individual similarity vs similarity in outcomes');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-supplement",
   "metadata": {},
   "source": [
    "### Rejoin the similarity pairs to the original data\n",
    "\n",
    "* Analyze the (lack of) fairness in terms of original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin(dm, df):\n",
    "    \n",
    "    # join similarities with original data\n",
    "    # prefix=1,2 denote data attached to each individual in pair\n",
    "    m = (\n",
    "        dm\n",
    "        .merge(df, left_on='idx_1', right_index=True)\n",
    "        .merge(df, left_on='idx_2', right_index=True, suffixes=['_1', '_2'])\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    \n",
    "    # extract the data associated to the higher COMPAS score\n",
    "    # when an unfair decision is made, this is the individual impacted most by decision\n",
    "    cols1 = m['decile_score_1'] < m['decile_score_2']\n",
    "    cols2 = m['decile_score_1'] > m['decile_score_2']\n",
    "    repl_cols = ['race', 'decile_score', 'is_recid', 'c_charge_degree', 'c_charge_desc', 'sex', 'age']\n",
    "\n",
    "    for c in repl_cols:\n",
    "        m.loc[cols1, c] = m.loc[cols1, '%s_2' % c]\n",
    "        m.loc[cols2, c] = m.loc[cols2, '%s_1' % c]\n",
    "    \n",
    "    # Calculate if a FP or FN occurred within the pair\n",
    "    FP = ((m.decile_score >= 5) & (m.is_recid == 0)).astype(int)\n",
    "    FN = (\n",
    "        ((m['decile_score_1'] < m['decile_score_2']) & (m['decile_score_1'] < 5) & (m['is_recid_1'] == 1)) |\n",
    "        ((m['decile_score_2'] < m['decile_score_1']) & (m['decile_score_2'] < 5) & (m['is_recid_2'] == 1))\n",
    "    ).astype(int)\n",
    "    \n",
    "    m = m.assign(FP=FP, FN=FN) \n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rejoin(dm, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-voluntary",
   "metadata": {},
   "source": [
    "### Individuals distance ~0.31 apart\n",
    "\n",
    "* Note, that not everyone is similar in the same way!\n",
    "* Larger difference in age could be made up by a smaller distance in prior_count, for example\n",
    "* Which variables have largest influence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-emperor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    r.loc[\n",
    "        (r.d < 0.32) & (r.d > 0.30), \n",
    "        [c for c in r.columns if (('_1' in c) or ('_2' in c))]\n",
    "    ].loc[:,[c for c in r.columns if c[:-2] in metric_vars]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-standard",
   "metadata": {},
   "source": [
    "### Very unfair pairs:\n",
    "* individuals nearly identical (< 0.02)\n",
    "* outcomes very different (> 0.5)\n",
    "\n",
    "What are the (differing) characteristics of these pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-executive",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rejoin(\n",
    "    unfairs[(unfairs['d'] < 0.11) & (unfairs['D'] > 0.5)],\n",
    "    df\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-offering",
   "metadata": {},
   "source": [
    "### Focus on unfair decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = rejoin(unfairs, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    a.race.value_counts(normalize=True).rename('unfairly treated'),\n",
    "    recidivism.race.value_counts(normalize=True).rename('population')\n",
    "], axis=1).plot(kind='bar', title='distibution of impacts from unfair decisions by race');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-cleaning",
   "metadata": {},
   "source": [
    "### Average difference in outcomes by race (among unfair decisions)\n",
    "\n",
    "* Scores of Black defendants among unfair decisions are higher than those of White defendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=a, x='race', y='D');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-antibody",
   "metadata": {},
   "source": [
    "### Recidivism Rate of by race among the higher of the COMPAS scores in unfair decisions\n",
    "\n",
    "* The difference in Black vs White defendants may be due to the high FN rate in White population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=a, x='race', y='is_recid');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict = {'Other': 'b', 'Caucasian': 'orange', 'African-American': 'green', 'Hispanic': 'red', 'Asian': 'purple', 'Native American': 'brown'}\n",
    "jittered = a.assign(\n",
    "    D=(a['D'] + np.random.normal(0,0.02, size=a.shape[0]))\n",
    ")\n",
    "\n",
    "ax = jittered.plot(kind='scatter', x='d', y='D', c=a['race'].replace(cdict), alpha=0.4, title='(d,D) by Race');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    recidivism.c_charge_desc.value_counts(normalize=True).iloc[:10].rename('population'), \n",
    "    a.c_charge_desc.value_counts(normalize=True).iloc[:10].rename('unfairly treated')\n",
    "], axis=1).plot(kind='bar', title='Distribution of charge description');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    recidivism.c_charge_degree.value_counts(normalize=True).iloc[:10].rename('population'), \n",
    "    a.c_charge_degree.value_counts(normalize=True).iloc[:10].rename('unfairly treated')\n",
    "], axis=1).plot(kind='bar', title='Charge Degree (Felony vs Misd) distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-solution",
   "metadata": {},
   "source": [
    "### Average age by race/sex in unfair decisions\n",
    "* Pretty even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=a, x='race', y='age', hue='sex');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-brand",
   "metadata": {},
   "source": [
    "### False Positive Rate by race in unfair decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=a, x='race', y='FP');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-learning",
   "metadata": {},
   "source": [
    "### False Negative Rate by race in unfair decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=a, x='race', y='FN');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-extent",
   "metadata": {},
   "source": [
    "### Other metric definitions\n",
    "\n",
    "* What tweaks can you think of?\n",
    "* Make your own and try them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxSimilarityMetric(CompasMetricBase):\n",
    "    \n",
    "    def dist(self, ser1, ser2):\n",
    "        \n",
    "        # absolute difference\n",
    "        dnum = np.abs((ser1.loc[self.nums] - ser2.loc[self.nums])).max()\n",
    "        \n",
    "        # exact-match\n",
    "        dcat = (1 - (ser1.loc[self.cats] == ser2.loc[self.cats]).astype(int)).max()\n",
    "        \n",
    "        return max(dnum, dcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "recidivism.c_charge_desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompasMetricBow(object):\n",
    "    \n",
    "    def __init__(self, df, metric_vars):\n",
    "        \n",
    "        self.vars = metric_vars\n",
    "        \n",
    "        df = df[metric_vars]\n",
    "        self.nums = df.dtypes.loc[lambda x:(x == 'float') | (x == 'int')].index\n",
    "        self.cats = df.dtypes.loc[lambda x:x == 'object'].index\n",
    "        \n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        dv = CountVectorizer()\n",
    "        bow = dv.fit_transform(df['c_charge_desc'].replace(np.nan, '')).todense()\n",
    "        self.bow = pd.DataFrame(bow, columns=sorted(dv.vocabulary_, key=lambda x:x[1]))\n",
    "        \n",
    "        return\n",
    "    \n",
    "        \n",
    "    def dist(self, ser1, ser2):\n",
    "\n",
    "        # absolute difference\n",
    "        dnum = np.abs((ser1.loc[self.nums] - ser2.loc[self.nums])).sum()\n",
    "        \n",
    "        # exact-match for charge degree (M vs F)\n",
    "        dcat1 = (1 - (ser1.loc[['c_charge_degree']] == ser2.loc[['c_charge_degree']]).astype(int)).sum()\n",
    "        \n",
    "        # bow for charge description\n",
    "        bow = self.bow\n",
    "        i1, i2 = ser1.name, ser2.name\n",
    "        dot = (bow.loc[i1] * bow.loc[i2]).sum()\n",
    "        N1 = np.sqrt((bow.loc[2]**2).sum())\n",
    "        N2 = np.sqrt((bow.loc[3]**2).sum())\n",
    "        dcat2 = dot / (N1 * N2)\n",
    "        \n",
    "        return (dnum + dcat1 + dcat2) / len(self.vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompasMetricLuckEgal(object):\n",
    "    \n",
    "    def __init__(self, df, metric_vars):\n",
    "        \n",
    "        self.vars = metric_vars\n",
    "        \n",
    "        df = df[metric_vars + ['race']]\n",
    "        self.nums = df.dtypes.loc[lambda x:(x == 'float') | (x == 'int')].index\n",
    "        self.cats = df.dtypes.loc[lambda x:x == 'object'].index\n",
    "        \n",
    "        cdfs = {}\n",
    "        for c in d.nums:\n",
    "            cdfs[c] = (\n",
    "                df\n",
    "                .groupby('race')[c]\n",
    "                .apply(self._calculate_cdf)\n",
    "                .reset_index()\n",
    "                .pivot(index='level_1', columns='race', values=c)\n",
    "                .fillna(1.0)\n",
    "            )\n",
    "            \n",
    "            if 'Asian' not in cdfs[c].columns:\n",
    "                cdfs[c]['Asian'] = 1.0\n",
    "            \n",
    "        self.cdfs = cdfs\n",
    "        \n",
    "        return\n",
    "    \n",
    "        \n",
    "    def dist(self, ser1, ser2):\n",
    "\n",
    "        dnum = 0\n",
    "        for c in self.nums:\n",
    "            cdf = self.cdfs[c]\n",
    "            p1 = cdf.loc[ser1[c], ser1['race']]\n",
    "            p2 = cdf.loc[ser2[c], ser2['race']]\n",
    "            dnum = dnum + np.abs(p1 - p2)\n",
    "        \n",
    "        # exact-match for charge degree (M vs F)\n",
    "        dcat = (1 - (ser1.loc[d.cats] == ser2.loc[d.cats]).astype(int)).sum()\n",
    "    \n",
    "        return (dnum + dcat) / len(self.vars)\n",
    "    \n",
    "    \n",
    "    def _calculate_cdf(self, col):\n",
    "\n",
    "        cdf = col.value_counts(normalize=True).sort_index().cumsum()\n",
    "        reindexed_cdf = cdf.reindex(\n",
    "            pd.RangeIndex(\n",
    "                start=cdf.index.min(), \n",
    "                stop=(cdf.index.max() + 1)\n",
    "            ), method='nearest')\n",
    "\n",
    "        return reindexed_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-circus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

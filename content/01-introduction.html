
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Introduction &#8212; Fairness &amp; Algorithmic Decision Making</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Bibliography" href="bibliography.html" />
    <link rel="prev" title="Fairness and Algorithmic Decision Making" href="../intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Fairness & Algorithmic Decision Making</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Fairness and Algorithmic Decision Making
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   2. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/01-introduction.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decisions-informed-by-data">
   1.1. Decisions informed by data
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithmic-decisions">
     1.1.1. Algorithmic decisions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-fingerprint-unlock">
       1.1.1.1. Example: fingerprint unlock
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-tay-the-racist-chatbot">
       1.1.1.2. Example: Tay the racist chatbot
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#uses-of-algorithmic-decisions">
   1.2. Uses of algorithmic decisions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#allocative-decisions">
   1.3. Allocative decisions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representational-decisions">
   1.4. Representational decisions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#judging-the-quality-of-a-decision">
   1.5. Judging the quality of a decision
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#context-who-is-judging-the-quality">
     1.5.1. Context: who is judging the quality?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-machine-learning-pipeline">
   1.6. The machine learning pipeline
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="introduction">
<h1><span class="section-number">1. </span>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>This course broadly approaches the issue of fairness in algorithmic
decision making for the <em>practicing data scientists</em>. We emphasize
reasoning about data-driven (algorithmic) decision making and its
interaction with society. This ability to reason helps the data
science practitioner engage in productive discussion with those their
models affect, ultimately resulting in more people benefiting from
the work.</p>
<p>One increasingly popular approach to studying fairness in the
application of statistical models to algorithmic decision making is to
fix an observable, quantitative notion of fairness and treat that
quantity as a constraint to include when fitting the model. This
approach intends to ‘fix’ algorithmic bias. While such observational
notions of fairness are useful for identifying differences between
groups in collected data, it misses much: Who is missing from the
data? Why do such differences exist? Are there other groups that need
simultaneous consideration? Are there other notions of fairness that
need simultaneous consideration? Is how the model’s output used fair?
The course aims to empower the data scientist to holistically
discuss their data-driven systems and how it interacts with the
populace it affects. This way, the data scientist can represent their
work and how it’s used in the public sphere – and do their best for
their fellow human beings.</p>
<p>The topics covered in the course consist of:</p>
<ol class="simple">
<li><p>An overview of concepts related to fairness (such as equality,
justice, bias, and discrimination) and the contexts under which
those concepts are considered (such as legal or social).</p></li>
<li><p>Discussion of measurement and data collection and its relationship
to concepts of power.</p></li>
<li><p>Approaches to identifying existing (various notions of) inequity
in data, while probing the limits to these approaches. Such
approaches include using parity measures for measuring
discrepancies across groups, causal models, and individual
comparisons.</p></li>
<li><p>A close examination of how unfairness may appear across a typical
machine learning pipeline.</p></li>
<li><p>Understanding ways in which the decision making process transforms
a model into a generative process with long term effects like
feedback loops.
6  Applying the analysis of feedback loops to matching in online
markets (Ad serving, gig economy, recommendations).</p></li>
<li><p>Approaching the problem of fairness in “decision agnostic” models
that are later used in unforeseen ways (e.g. vector embedding, 3rd
party APIs).</p></li>
</ol>
<div class="section" id="decisions-informed-by-data">
<h2><span class="section-number">1.1. </span>Decisions informed by data<a class="headerlink" href="#decisions-informed-by-data" title="Permalink to this headline">¶</a></h2>
<p>What is algorithmic decision making? In practice, it’s an automated
scheme for making decisions <em>using data</em>. Policy makers, with help
from researchers in the social sciences, humanities, and sciences,
have been making decisions informed by data for centuries. Thus, in
order to study fairness in algorithmic decision making, we should
first examine how people make decisions informed by data and the ways
in which those decisions affect the fairness of the outcomes.</p>
<p>The first half of the course studies frameworks for understanding
concepts related to fairness, identifying inequities in data, and
examining when those inequities may have resulted from policy
decisions. We will draw on work from Philosophy, Sociology, Economics,
and Computer Science. While identifying and explaining inequity in
observed data is more general than the study of fairness in
algorithmic decisions, it’s a necessary first step before going on to
examine the added dynamics that algorithmic decisions bring to the
problem.</p>
<p>The ability of a data scientist to surface inequities in the data they
study is not only necessary for understanding the consequences of
their (algorithmic) decisions, it’s also a super-power waiting to be
used for good. Such studies raise awareness of inequities, help build
movements demanding change, and aid your fellow humans in bettoring
their lives.</p>
<div class="section" id="algorithmic-decisions">
<h3><span class="section-number">1.1.1. </span>Algorithmic decisions<a class="headerlink" href="#algorithmic-decisions" title="Permalink to this headline">¶</a></h3>
<p>What is the difference between an algorithmic decision and a
traditional decision?</p>
<p>An algorithm is merely a set of rules used to calculate some
output. As such, unfair treatment by a decision making algorithm
simply codifies unfairness present in the logic used to define the
algorithm. The algorithmic component enables such unfair treatment to
scale to larger groups of people.</p>
<p>One example of a biased algorithm from 1934 (predating computers) is
redlining in the United States. The Federal Housing Administration
(FHA) issued guidelines for issuing mortgages based on place of
residence. This process was algorithmic in that the FHA created maps
of cities that classified neighborhoods by ‘estimated riskiness’ of
mortgage loans and distributed them to lending organizations as
guidelines for making loaning decisions. These risk scores largely
reflected a loan applicant’s race and broadly enforced discriminatory
practices that encouraged country-wide racial segregation and urban
neglect. Eighty-five years later, the United States still suffers from
the consequences of this racist policy and still disagrees with how to
deal with its aftermath. (Learn more about redlining here: <span id="id1">[<a class="reference internal" href="bibliography.html#id5"><span>Gro17</span></a>]</span>).</p>
<p>So what is new then, about algorithmic decision making?</p>
<p>With the proliferation of computers into our lives, algorithms are
responsible for making more of our daily decisions and the logic
behind those decisions are becoming increasingly
complicated. Moreover, when confronted with such opacity behind
algorithmic decisions, people tend to assume that the decision is
objective and correct.</p>
<div class="section" id="example-fingerprint-unlock">
<h4><span class="section-number">1.1.1.1. </span>Example: fingerprint unlock<a class="headerlink" href="#example-fingerprint-unlock" title="Permalink to this headline">¶</a></h4>
<p>Americans unlock their phones 52 times per day on average
<span id="id2">[<a class="reference internal" href="bibliography.html#id3"><span>Del18</span></a>]</span>. One feature that software companies developed
to ease the friction of phone usage, while maintaining the security of
users, is the allowing users to unlock their phone with a
fingerprint. Users who do heavy work with their hands noticed that
this feature didn’t work reliably <span id="id3">[<a class="reference internal" href="bibliography.html#id4"><span>spinedoc7714</span></a>]</span>.</p>
<p><img alt="fingerprint" src="../_images/fingerprint.jpeg" /></p>
<p><em>How was this decision making feature designed?</em></p>
<p>From the designer’s perspective, given a fingerprint, the phone must
decide whether it matches a stored fingerprint well enough to decide
to unlock the phone. The quantitative threshold for “close enough to
match” depends on how fingerprints are quantitatively compared and
calibrated using a sample of collected fingerprint data.</p>
<p><em>How is this fingerprint feature unfair?</em></p>
<p>The fingerprint unlock was rendered unusable to those who make heavy
use of their hands, a large group of potential users with lower
incomes on average.</p>
<p><em>How did this bias occur?</em></p>
<p>On the one hand, the dataset used to calibrate the algorithm likely
came from a sample of existing smart phone users, a demographic that
does not skew toward employment in manual labor. Thus, it was “good
enough to use” as soon as it passed muster for a specific group.</p>
<p>On the other hand, one could make the argument that making decisions
based on worn fingers is a harder problem to solve than for uncallused
fingers. In this case, you might ask why the designers felt that this
was the best feature to introduce to solve this problem in the first
place. This example brings up the value of the diversity of
perspectives during idea development and design.</p>
<p><em>What are the effects and impacts of this design?</em></p>
<p>Although the impact of a single mistaken decision of unlocking the
phone is small, the high frequency of the occurence adds up.</p>
<p>One group leads a frictionless technological life, complete with
same-day free shipping, while the other constantly struggles with
life’s small difficulties (on top of their larger struggles, which
likely placed them in this group to begin with).</p>
<p>From perspective of security-policy, studies show that such friction
drastically increase the likelihood that a phone user forgoes security
measures all-together. Thus, on a group-level, the unfairness of the
design of this feature also leaves those in this disadvantaged group
more exposed to bad actors.</p>
<p>Lastly, this bias also has negative impact on the software
company. Testing the quality of the feature on the phone’s
current user-base ensures that the feature is useful for the company’s
current customers. However, the mobile phone business cares about
<em>growth</em>; neglecting to consider a large segment of the population as
potential customers is bad for business.</p>
</div>
<div class="section" id="example-tay-the-racist-chatbot">
<h4><span class="section-number">1.1.1.2. </span>Example: Tay the racist chatbot<a class="headerlink" href="#example-tay-the-racist-chatbot" title="Permalink to this headline">¶</a></h4>
<p>In 2016, Microsoft released a chatbot named Tay as a public experiment
in conversational learning in a natural environment. The Tay’s
algorithm would tweet back responses to questions on Twitter; these
responses were her decisions and she made approximately 6000/hr. These
repsonses were created by a large, complicated, NLP model trained on
publicly available conversatinal text from the internet, along with
what the algorithm ‘learned’ from real-time questions and
responses. In less than a day after Tay was released on Twitter, she
was commonly tweeting racist, abusive content; Microsoft suspended
Tay’s account.</p>
<p><img alt="Tay" src="../_images/Tay_bot_logo.jpg" /></p>
<p><em>In what ways is this algorithm unfair?</em></p>
<p>Releasing a state-of-the-art, highly publicized, chatbot that streams
invective both directs harm at the individuals with whom she
converses, as well as amplifies harmful, racist tropes among the
public.</p>
<p><em>How did this bias occur?</em></p>
<p>There’s some possibility that Tay learned some of these insults from
the data on which the model was trained (public text from the internet
includes content like this). However, most of this language was taught
to the chatbot in real-time, interacting with twitter users who
actively their hate to Tay. This sort of rapid, uncontrolled feedback
from the chatbot’s environment is a common problem associated to the
speed and scale of algorithmic decision making.</p>
<p>Read more about Tay here: <span id="id4">[<a class="reference internal" href="bibliography.html#id6"><span>Sch19</span></a>]</span>.</p>
<hr class="docutils" />
<p>Another difference the ‘algorithmic’ adds to decision making is the
potential for exhaustive record-keeping and careful scrutiny. The
infrastructure required for algorithmic decision making is the same
infrastructure needed to collect the records for informing and
auditing those decisions <span id="id5">[<a class="reference internal" href="bibliography.html#id2"><span>KLMS19</span></a>]</span>. This aspiration
does not come automatically; it demands a respect for the effects a
biased system may have and effort to use that information to improve
the world.</p>
</div>
</div>
</div>
<div class="section" id="uses-of-algorithmic-decisions">
<h2><span class="section-number">1.2. </span>Uses of algorithmic decisions<a class="headerlink" href="#uses-of-algorithmic-decisions" title="Permalink to this headline">¶</a></h2>
<p>While the encoding of fairness in the models and logic that inform
decision making algorithms receive scrutiny, the decisions themselves
are often passed off as others problem. Data merely informs a
decsision; what that decision is requires just as much care.</p>
<p>The sorts of decisions made by algorithms studied in this course
largely fall into two categories: allocative and representational.</p>
<ol class="simple">
<li><p>Allocative decisions determine whether or not to allocate resources
or opportunities of its subjects.</p></li>
<li><p>Representational decisions surface the most relevant features of
its subject and returns that representation.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The use of ‘allocative’ and ‘representational’ here is
nonstandard; these terms are typically used to categorize the kinds of
<em>harms</em> an algorithm inflicts. I think this perspective is useful, as
it naturally asks what the ideal outcome of such a decision might be.</p>
</div>
</div>
<div class="section" id="allocative-decisions">
<h2><span class="section-number">1.3. </span>Allocative decisions<a class="headerlink" href="#allocative-decisions" title="Permalink to this headline">¶</a></h2>
<p>The allocation of scarce resources is the central problem studied in
Economics and has been studied for as long as nation states have
existed. In the growing literature of fairness and algorithmic
decision making, the study of allocative decisions compose the bulk of
the research. These decisions include high impact decisions of how to
allocate government resources and whether to grant someone their
freedom.</p>
<p>We will examine case-studies of different allocative algorithms
throughout the course. Below is a list of examples that illustrate the
breadth of such decisions:</p>
<ol class="simple">
<li><p>Criminal justice (granting bail, granting parole, sentencing, crime
prediction)</p></li>
<li><p>Human resources (screening for hiring, school admissions, promotion)</p></li>
<li><p>The serving of online advertisments</p></li>
<li><p>Loan granting (loan calculation, credit scoring, mortgage lending)</p></li>
<li><p>Fraud and abuse detection (suspensions on social networks)</p></li>
<li><p>Prioritization of medical services (access to medicines and
procedures, triage)</p></li>
</ol>
<p>In many of these examples, algorithms have the opportunity to improve
current policies that mistreat underserved communities. However, this
takes thoughtfulness and care by those designing the system, and
meaningful participation of <em>all</em> the communities it aims to serve.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If some of these don’t seem like they allocate concrete resources,
that’s because they may allocate opportunities, or other assets with
measurable utility. We will discuss in the next lecture.</p>
</div>
</div>
<div class="section" id="representational-decisions">
<h2><span class="section-number">1.4. </span>Representational decisions<a class="headerlink" href="#representational-decisions" title="Permalink to this headline">¶</a></h2>
<p>Representational algorithms summarize a concept or individual in
service of a question or task. The most common example of
representational algorithms are those related to information
retrieval and recommender systems. For example, given a search term,
the algorithms must decide what the most relevant information is
related to that term (what information ‘represents’ the term).</p>
<p>Such algorithms include:</p>
<ul class="simple">
<li><p>(Textual) search engines</p></li>
<li><p>Autocomplete</p></li>
<li><p>Image search</p></li>
<li><p>Language translation</p></li>
<li><p>Generative language models (like Tay the chatbot)</p></li>
<li><p>Recommender Systems</p></li>
<li><p>Information feeds (Facebook Timeline, Twitter Feed)</p></li>
</ul>
<p>Such algorithms influence users’ access to information and shape
public discource.</p>
</div>
<div class="section" id="judging-the-quality-of-a-decision">
<h2><span class="section-number">1.5. </span>Judging the quality of a decision<a class="headerlink" href="#judging-the-quality-of-a-decision" title="Permalink to this headline">¶</a></h2>
<p>Ideally, behind every algorithmic decision making system is a
developer that analyzes the quality of the decisions. For model-based
decisions, a data scientist will curate a test-set and determine the
performance of the model – metrics like accuracy, precision, and
recall. Such analyses will approximate the average quality of the
decisions before the developer deploys the system.</p>
<p>While the data scientist has incentives to do this analysis well,
there are serious shortcomings to such a cursory analysis:</p>
<ul class="simple">
<li><p>The primary metric used to evaluate performance likely reflects the
business’ bottom-line. Are there other metrics that measure
real-life impact on users?</p></li>
<li><p>Performance metrics typically measure <em>average</em> model quality. Do
poor decisions have outsized impact on specific individuals or
smaller groups of users? What is the worst case scenario of a poor
decision? Is the decision even worth making at all?</p></li>
<li><p>Are certain groups of users more poorly served by the model than
most? Is the poor performance random or systematically biased to
certain groups of users?</p></li>
</ul>
<p>Answering all these questions not on helps develop a <em>fairer</em> decision
making algorithm, but a <em>better</em> one as well. To help guide such
analyses, researchers are developing reporting tools for auditing
modeling pipelines <span id="id6">[<a class="reference internal" href="bibliography.html#id7"><span>GMV+18</span></a>,<a class="reference internal" href="bibliography.html#id8"><span>MWZ+18</span></a>]</span>. However, these
tools merely structure an investigation; the developer must still
think through the process.</p>
<p>The next chapter will examine different frameworks for understanding
model quality and sociotechnical systems, beginning with long-studied
topics of fairness, equality, and justice.</p>
<div class="section" id="context-who-is-judging-the-quality">
<h3><span class="section-number">1.5.1. </span>Context: who is judging the quality?<a class="headerlink" href="#context-who-is-judging-the-quality" title="Permalink to this headline">¶</a></h3>
<p>Another thing to consider is the context under which the decisions
occur. Are there legal implications to the decisions? There are
precise notions of fairness in law (e.g. disparate impact) that may
differ from public opinion. The consequences of running afoul of these
notions will also be quite different: breaking the law may lead to
prosecution, whereas releasing an unfair algorithm in the eyes of the
public may lead to a boycott of your product.</p>
</div>
</div>
<div class="section" id="the-machine-learning-pipeline">
<h2><span class="section-number">1.6. </span>The machine learning pipeline<a class="headerlink" href="#the-machine-learning-pipeline" title="Permalink to this headline">¶</a></h2>
<p>The second half of the course focuses on ways in which bias and
unfairness manifests itself in a decision making system: how it
propogates it, creates it, and possibly amplifies it.</p>
<p>A simplified schematic of a data-driven algorithmic decision making
pipeline looks like this:</p>
<p><img alt="pipeline" src="../_images/simple_pipeline.png" /></p>
<p>Bias and unfairness can influence this pipeline at any step, as it’s
informed by data measured by humans and designed by human developers,
each of which is influenced by its own biases.</p>
<p>At the data step:</p>
<ul class="simple">
<li><p>Does the data represent the correct population?</p></li>
<li><p>Does the data capture existing biases and power structures?</p></li>
<li><p>Do measurements in the data capture what’s needed to make decision?</p></li>
</ul>
<p>Moving from the data step to model step:</p>
<ul class="simple">
<li><p>Does the data cleaning introduce bias?</p></li>
<li><p>Are missing values and imputation affecting the data?</p></li>
<li><p>Is data being aggregated in ways that erases needed information for
subpopulations? (E.g. Simpson’s Paradox).</p></li>
</ul>
<p>At the model step:</p>
<ul class="simple">
<li><p>Does the label capture true outcomes or biased proxies?</p></li>
<li><p>Is the loss function reflect what you are trying to optimize?</p></li>
<li><p>When choosing bias over variance, does that bias affect certain
subpopulations over others?</p></li>
</ul>
<p>Moving from the model step to decision step:</p>
<ul class="simple">
<li><p>Does the calibration of the model output treat individuals fairly?</p></li>
<li><p>Do certain model outcomes (e.g. false positives/negatives) affect
the decisions differently?</p></li>
</ul>
<p>At the decision step:</p>
<ul class="simple">
<li><p>Is the decision itself ethical and fair? What if the decision is
wrong?</p></li>
<li><p>Are the decisions being interpreted and used correctly?</p></li>
<li><p>Can those affected by the decisions ‘contest’ the decision in some
way?</p></li>
</ul>
<p>Moving from the decision step to the data step:</p>
<ul class="simple">
<li><p>How do decisions affect later models? Are certain groups more likely
to experience poor decisions in later models?</p></li>
<li><p>If the decisions only capture partial ground truth, is this
introducing bias in future labels?</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../intro.html" title="previous page">Fairness and Algorithmic Decision Making</a>
    <a class='right-next' id="next-link" href="bibliography.html" title="next page"><span class="section-number">2. </span>Bibliography</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Aaron Fraenkel<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>
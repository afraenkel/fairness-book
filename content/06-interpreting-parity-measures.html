
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>6. Interpreting Parity Measures &#8212; Fairness &amp; Algorithmic Decision Making</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Score Functions, Calibration, and Fairness" href="07-score-functions.html" />
    <link rel="prev" title="5. Parity Measures" href="05-parity-measures.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Fairness & Algorithmic Decision Making</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Fairness and Algorithmic Decision Making
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01-introduction.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02-frameworks.html">
   2. What is Fairness?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-harms.html">
   3. Harm, Discrimination, and Measurement
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-compas.html">
   4. COMPAS Recidivism Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-parity-measures.html">
   5. Parity Measures
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Interpreting Parity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-score-functions.html">
   7. Score Functions, Calibration, and Fairness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   8. Bibliography
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/06-interpreting-parity-measures.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/afraenkel/fairness-book/main?urlpath=tree/./content/06-interpreting-parity-measures.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review-of-frameworks-for-fairness">
   6.1. Review of Frameworks for Fairness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#translating-the-components-of-an-algorithmic-decision">
   6.2. Translating the Components of an Algorithmic Decision
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rawlsian-feo">
     6.2.1. Rawlsian FEO
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#luck-egalitarianism">
     6.2.2. Luck Egalitarianism
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretations-of-trade-off-using-equality-of-opportunity">
   6.3. Interpretations of trade-off using Equality of Opportunity
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="interpreting-parity-measures">
<h1><span class="section-number">6. </span>Interpreting Parity Measures<a class="headerlink" href="#interpreting-parity-measures" title="Permalink to this headline">¶</a></h1>
<p>This lecture will follow the treatment in <em>A Moral Framework for Understanding of Fair ML through Economic Models of Equality of Opportunity</em> <span id="id1">[<a class="reference internal" href="bibliography.html#id12"><span>HLG+19</span></a>]</span>.</p>
<p>This section presents one possible translation of the parity criteria from the last lecture into the frameworks of Rawlsian Equality of Opportunity and Luck Egalitarianism. This translation is only one possible attempt at mapping these concepts and does so in an <em>observational</em> way. It attempts to make clear underlying assumptions of fairness in these observational criteria.</p>
<div class="section" id="review-of-frameworks-for-fairness">
<h2><span class="section-number">6.1. </span>Review of Frameworks for Fairness<a class="headerlink" href="#review-of-frameworks-for-fairness" title="Permalink to this headline">¶</a></h2>
<p>We briefly review the (quantitative) definitions for Rawlsian Equality of Opportunity and Luck Egalitarianism given in lecture 2.</p>
<p>Suppose that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(c\)</span> stands for circumstances that capture factors beyond one’s control (e.g. circumstances of birth, or that brought on by luck).</p></li>
<li><p><span class="math notranslate nohighlight">\(e\)</span> are actions that follow from ones choices and character (‘effort’).</p></li>
<li><p><span class="math notranslate nohighlight">\(u\)</span> is the utility</p></li>
<li><p>An (allocative) policy <span class="math notranslate nohighlight">\(\phi\)</span> induces a distribution of utility among a population.</p></li>
<li><p><span class="math notranslate nohighlight">\(F^\phi(.|c, e)\)</span> is the cumulative distribution of utility under policy <span class="math notranslate nohighlight">\(\phi\)</span>, at fixed effort level <span class="math notranslate nohighlight">\(e\)</span> and circumstance <span class="math notranslate nohighlight">\(c\)</span>.</p></li>
</ul>
<p><strong>Rawlsian Equality of Opportunity</strong> can be translated into the language above as follows. We say a policy <span class="math notranslate nohighlight">\(\phi\)</span> satisfies Rawlsian Equality of Opportunity if for all circumstances <span class="math notranslate nohighlight">\(c,c'\)</span> and all efffort levels <span class="math notranslate nohighlight">\(e\)</span>,</p>
<div class="math notranslate nohighlight">
\[
F^\phi(.|c,e) = F^\phi(.|c', e)
\]</div>
<p>This notion supposes a view of effort that makes sense to compare between any two individuals. Effort is inherent to an individual and not impacted by circumstance <span class="math notranslate nohighlight">\(c\)</span>. Also recall that effort captures a lot more than simple effort (according to Rawls, it captures talent, ambition and other natural characteristics as effort).</p>
<p><strong>Luck Egalitarian Equality of Opportunity</strong> can be translated into the language above as follows. Suppose that <span class="math notranslate nohighlight">\(\pi\)</span> is the <span class="math notranslate nohighlight">\(\pi\)</span>th quantile of the distribution of effort of individuals under circumstance <span class="math notranslate nohighlight">\(c\)</span>. Then a policy  satisfies Luck Egalitarian EO if for all <span class="math notranslate nohighlight">\(\phi\in[0,1]\)</span> and any two circumstances <span class="math notranslate nohighlight">\(c, c'\)</span></p>
<div class="math notranslate nohighlight">
\[
F^\phi(.|c,\pi) = F^\phi(.|c', \pi)
\]</div>
<p>In the deciding admission to college, this would be like comparing those at top 10% of their high-school class. Students at different high schools likely have different absolute academic credentials. This policy considers the environment (circumstance) the student came from when comparing effort levels.</p>
</div>
<div class="section" id="translating-the-components-of-an-algorithmic-decision">
<h2><span class="section-number">6.2. </span>Translating the Components of an Algorithmic Decision<a class="headerlink" href="#translating-the-components-of-an-algorithmic-decision" title="Permalink to this headline">¶</a></h2>
<p>Now we will translate the formalism above into the context of an algorithmic decision making system. Consider the notation from the previous lecture:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> are relevant attributes (e.g. those reasonable for making decisions)</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span> are irrelevant attributes (e.g. race)</p></li>
<li><p><span class="math notranslate nohighlight">\(Y\)</span> is the outcome or <em>true label</em>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{Y} = C(X, A)\)</span> is an allocation by a classifier.</p></li>
</ul>
<p>The utility that an algorithmic decision imparts may be described as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d\in[0,1]\)</span> is an individual’s <em>effort-based utility</em>. This is the utility they receive based on relevant factors made by the individual. This utility is not directly observable.</p></li>
<li><p><span class="math notranslate nohighlight">\(a\in[0,1]\)</span> is the <em>actual utility</em> the individual receives subsequent to receiving allocation <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(u\)</span> is the utility or advantage the individual earns as a result of being subject to the decision <span class="math notranslate nohighlight">\(C\)</span>. We assume <span class="math notranslate nohighlight">\(u = a - d\)</span> in these notes.</p></li>
</ul>
<p>The overall utility <span class="math notranslate nohighlight">\(u\)</span> is what you want to equalize across individuals in similar circumstances. Note:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(u = 0\)</span>, then <span class="math notranslate nohighlight">\(a = d\)</span> and effort directly matches what’s given.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(u &gt;0\)</span>, then <span class="math notranslate nohighlight">\(a &gt; d\)</span> and the individual benefits from the decision (beyond effort).</p></li>
<li><p>If <span class="math notranslate nohighlight">\(u&lt;0\)</span>, then <span class="math notranslate nohighlight">\(a &lt; d\)</span> and the individual is harmed by the decision (taking effort into account).</p></li>
</ul>
<p>Equality of Opportunity, according to Heidari et al., requires individuals with similar effort have similar prospects of earning the advantage <span class="math notranslate nohighlight">\(u\)</span>. How similarity of effort is measured changes the specific conception of EO.</p>
<p>We can translate our formalism about EO into the language of machine learning:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Eqality of Opportunity</p></th>
<th class="head"><p>Machine Learning</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>policy <span class="math notranslate nohighlight">\(\phi\)</span></p></td>
<td><p>classifier <span class="math notranslate nohighlight">\(C\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>effort <span class="math notranslate nohighlight">\(e\)</span></p></td>
<td><p>effort-based utility <span class="math notranslate nohighlight">\(d\)</span></p></td>
</tr>
<tr class="row-even"><td><p>circumstance <span class="math notranslate nohighlight">\(c\)</span></p></td>
<td><p>irrelevant attributes <span class="math notranslate nohighlight">\(A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>utility <span class="math notranslate nohighlight">\(u\)</span></p></td>
<td><p>overall utility $u = (a - d)</p></td>
</tr>
</tbody>
</table>
<p>Note: by capturing circumstance with <span class="math notranslate nohighlight">\(A\)</span>, we are simplifying an individual notion of fairness to one about groups.</p>
<div class="section" id="rawlsian-feo">
<h3><span class="section-number">6.2.1. </span>Rawlsian FEO<a class="headerlink" href="#rawlsian-feo" title="Permalink to this headline">¶</a></h3>
<p>Rawlsian EO requires effort <span class="math notranslate nohighlight">\(d\)</span> to not be affected by irrelevant features <span class="math notranslate nohighlight">\(A\)</span> and the specific decision <span class="math notranslate nohighlight">\(C\)</span>. Let <span class="math notranslate nohighlight">\(F^C(.)\)</span> be the distribution of utility <span class="math notranslate nohighlight">\(u\)</span> across individuals under the allocative decisions made by <span class="math notranslate nohighlight">\(C\)</span>. The Rawlsian EOP translates to:</p>
<div class="math notranslate nohighlight">
\[
F^C(.|A = a, D = d) = F^C(.|A = b, D = d)
\]</div>
<p>That is, for fixed effort <span class="math notranslate nohighlight">\(d\)</span>, the distribution of utility does not depend on circumstance <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Many of the parity criteria from the last lecture are naturally described using Rawlsian Fair Equality of Opportunity, for different choices of effort. This allows us to clearly describe the assumptions we make when requiring a specific parity measure to hold.</p>
<p><strong>Equality of Odds</strong>. If the true binary label <span class="math notranslate nohighlight">\(Y\)</span> reflects an individual’s effort-based utility <span class="math notranslate nohighlight">\(D\)</span>, then Rawlsian FEO translates to equality of odds across protected groups. That is:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d = Y\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a = \hat{Y}\)</span></p></li>
<li><p>$u = (Y - \hat{Y})</p></li>
</ul>
<p>That is, equality of odds is a world where you receive exactly what you work for. Practically speaking however, if the true label identified in the training data doesn’t perfectly reflect a persons effort, this condition no longer holds!</p>
<p><strong>Demographic Parity</strong> is framed as Rawlsian EO by defining the effort-based utility as constant of <span class="math notranslate nohighlight">\(d = 1\)</span> and the actual utility to be the result of the decision <span class="math notranslate nohighlight">\(a = \hat{Y}\)</span>. This implies that each person has some intrinsic constant utility that’s independent of effort or circumstances. Such an assignment may be reasonable if you are considering the distribution of a good that equally deserved among all people (e.g. an inalienable right).</p>
<p><strong>Accuracy Parity</strong> is framed as Rawlsian EO by defining the effort-based utility as constant of <span class="math notranslate nohighlight">\(d = 0\)</span> and the actual utility to be the discrepancy between the decision and the true label <span class="math notranslate nohighlight">\((\hat{Y} - Y)^2\)</span>. This implies that individual has no intrinsic utility and all that matters is the correctness of the outcome.</p>
<p>The proofs of these equivalences are found in <span id="id2">[<a class="reference internal" href="bibliography.html#id12"><span>HLG+19</span></a>]</span>.</p>
</div>
<div class="section" id="luck-egalitarianism">
<h3><span class="section-number">6.2.2. </span>Luck Egalitarianism<a class="headerlink" href="#luck-egalitarianism" title="Permalink to this headline">¶</a></h3>
<p>With notation as before, define <span class="math notranslate nohighlight">\(\pi\)</span> to be the distribution of utility for individuals in  at the <span class="math notranslate nohighlight">\(\pi\)</span>th quantile of the distribution of effort-based utility. Equalizing opportunities means choosing the predictive model <span class="math notranslate nohighlight">\(C\)</span> to equalize the distribution of utility across types, at fixed levels of <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
F^C(.|A = a, \Pi = \pi) = F^C(.|A = b, \Pi = \pi)
\]</div>
<p><strong>Predictive Value Parity</strong> is framed as Luck Egalitarianism, by equating effort-based utility with the classifiers decision <span class="math notranslate nohighlight">\(d = \hat{Y}\)</span> and actual utility with the true outcome <span class="math notranslate nohighlight">\(a = Y\)</span>.</p>
<p>Here, the assumption made is that “differences in actual outcomes among equally risky individuals is mainly driven by arbitrary factors, such as brute luck”  <span id="id3">[<a class="reference internal" href="bibliography.html#id12"><span>HLG+19</span></a>]</span>. Thus predictive value parity assumes that the classifier’s assessment is assumed to capture effort, while differences between effort in the actual outcome are primarily due to circumstance.</p>
<p><em>Remark</em>: Predictive Value Parity cannot be framed as Rawlsian, as it requires conditioning on the decisions made by the classifier; it requires equal rates of true outcomes across similar predictions.</p>
<p>Different choices of <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(a\)</span> may lead to new parity measures that encode different values.  <span id="id4">[<a class="reference internal" href="bibliography.html#id12"><span>HLG+19</span></a>]</span> explores some of these at the end of the paper.</p>
</div>
</div>
<div class="section" id="interpretations-of-trade-off-using-equality-of-opportunity">
<h2><span class="section-number">6.3. </span>Interpretations of trade-off using Equality of Opportunity<a class="headerlink" href="#interpretations-of-trade-off-using-equality-of-opportunity" title="Permalink to this headline">¶</a></h2>
<p>Predictive value parity and equality of odds cannot hold simultaneously due to differences in moral assumptions encoded in effort-based utility <span class="math notranslate nohighlight">\(d\)</span>:</p>
<ul class="simple">
<li><p>Equality of odds assumes persons with similar true labels are equally accountable for their labels.</p></li>
<li><p>Predictive value parity assumes all persons with the same ‘risk’ (predicted label) are equally accountable for their predictions.</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="05-parity-measures.html" title="previous page"><span class="section-number">5. </span>Parity Measures</a>
    <a class='right-next' id="next-link" href="07-score-functions.html" title="next page"><span class="section-number">7. </span>Score Functions, Calibration, and Fairness</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Aaron Fraenkel<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>